# Обучение модели классификации комментариев

## Описание проекта

Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах.
То есть клиенты предлагают свои правки и комментируют изменения других.
Требуется инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.

## Ключевые слова проекта

`обработка естественного языка` `NLP` `классификация` `подбор гиперпараметров` `выбор модели МО`

## Навыки и инструменты

`исследовательский анализ данных` `Python` `Pandas` `NumPy` `Scikit-learn` `Re` `Nltk` `Tqdm` `Lightgbm` `Catboost`

## Результаты исследования

**В результате загрузки данных, установлено:**

1.  DataFrame содержит 159571 строк и 2 столбца
2.  В столбце «text» данные типа object, пропуски отсутствуют
3.  В столбце «toxic» данные типа int64, пропуски отсутствуют
4.  Явные дубликаты отсутствуют.

**В результате предобработки данных:**

1.  Выполнена лемматизация и очистка данных
2.  Исследован баланс классов (1:9)
3.  Выполнено кодирование и векторизация целевых признаков
4.  Выборка разделена на тренировочную и тестовую (75:25)

**В результате обучения и подбора лучших параметров моделей, установлено:**

1.  Модель: LogisticRegression(C=11, class_weight='balanced')
    * Параметры лучшей модели: {'C': 11, 'class_weight': 'balanced'}
    * F1: 0.762
2.  Модель: <catboost.core.CatBoostClassifier object at 0x7faabe99f090>
    * Параметры лучшей модели: {'depth': 1, 'n_estimators': 1}
    * F1: 0.264
3.  Модель: RandomForestClassifier(max_depth=2, n_estimators=2)
    * Параметры лучшей модели: {'max_depth': 2, 'n_estimators': 2}
    * F1: 0.059
4.  Модель: LGBMClassifier(max_depth=3, n_estimators=3)
    * Параметры лучшей модели: {'max_depth': 3, 'n_estimators': 3}
    * F1: 0.001

**В результате проверки качества модели на тестовой выборке:**

1.  Лучшая модель: LogisticRegression(C=11, class_weight='balanced')
2.  F1 наилучшей модели на тестовой выборке: 0.761
3.  Значение F1 удовлетворяет условию: F1 не меньше 0.75.
4.  Попытаемся улучшить качество предсказания других (кроме LogisticRegression()) моделей за счёт балансировки классов.

**В результате борьбы с дисбалансом и повторного подбора лучших параметров моделей (кроме LogisticRegression()), установлено:**

1.  Лучшая модель: LGBMClassifier(max_depth=3, n_estimators=3)
2.  F1 наилучшей модели на тестовой выборке: 0.373
3.  Лучший МНОЖИТЕЛЬ (хN), увеличивающий выборку: 5
4.  Прирост F1 порядка 11%, однако не удовлетворяет условию: F1 не меньше 0.75.

**РЕЗЮМЕ:**

1.  Лучшая модель: LogisticRegression(C=11, class_weight='balanced')
2.  F1 модели на тестовой выборке: 0.761
 
 ## Статус проекта
 `Завершен`
